# 27min开始到65min
import torch
'''
torch.randn初始化的数字，要如何改变才能让 x @ w它的值变成标准差为1 --- 要除以 fan_in ** 0.5，fan_in 两个矩阵相连的数字
比如x = torch.randn(1000, 10) w = torch.randn(10,100), y = x @ w, 那就要除以  10**0.5

论文细节《Diving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》
论文中提到了ReLU和PReLU，提到了如何精确地调整输入
backward也有提及，但是因为是个敞亮，不是很重要。

torch.nn.init.kaiming_normal_ 这个kaiming提的方法已经进入了标准库。mode一般就默认fan_in，影响不大
nonlinearity 非线性函数 比如 Leaky_relu / relu / tanh / sigmoid ...

gain值的计算
linear / identity / sigmoid / Conv{1,2,3}D (这是什么 --- 卷积层)== 1
tanh = 5/3
ReLU = 2**0.5
Leaky Relu (2 / (1 + 斜率**2))**0.5

为什么需要gain（增益）呢？
因为在计算隐藏层的时候，tanh就像ReLU一样，是一个压缩变换，它会把头尾的数据压缩在很小的空间内，所以我们要对抗一下这个压缩的动作，并且把
内容从新归一化回到单位标准差

但是这个对于激活值的梯度范围的初始化设置的文章是7年前提出的，在当时你必须很小心地去设置它，但是现在有更创新的方法，比如残差链接，多种归一化层
，更多优化器，Adam等。所以这些初始化的设置变得不是那么重要。
不过作者还是对w1 进行了  torch.randn(10, 1000) * 5 / 3 / fann**0.5的操作

40min 开始讲批量归一化层
hpreact = x @ w + b 这个就是所谓的隐藏层值
最开始人们希望hpreact的值能够绝对值不要太大或者太小，而是呈高斯分布，这样子的话后面tanh的时候不至于太大或者太小
因为
hpreact绝对值太小，tanh会不起作用，基本不会压缩。hpreact绝对值太大，tanh会所谓的饱和，让本来hpreact差距很大的数在tanh后挨在了一起，1或者-1附近

事实上我们希望它们至少在初始化的时候大致呈高斯分布，也就是零和均值和单位标准差。

--- 待确定
求梯度的时候会导致梯度为1，但是激活值太小这样就传递不了信息。
梯度会下降到0，导致反向传播的时候权重很难更新。

'''

test_p = torch.randn(10000).std()
print(test_p)
'''
40min - 54min
<<Batch Normalization: Acclerating Deep Network Training by Reducing Internal Covariate Shift>>
这篇论文提出的批量归一化的观点，可以让很大很深的神经网络训练的非常可靠有效
它们关键的观点是：
如果你希望隐藏层的值呈高斯分布，那为什么步直接取这些隐藏层状态并把它们归一化为高斯分布呢？

想象你现在是有x输入和w权重，每个输入都要和每个神经元作用得到输出
那在neuron0上所有输入的平均值就算的出来 ===> hpreact.mean(0, keepdim=True) 压缩第0维也就是纵向压缩，这里就是[1,3]
对应地，标准差也算的出来 ===> hpreact.std(0, keepdim=True)
这样做的好处是：每一个神经元及其激发率在这几个输入样本上，也就是这一批次中，都精确地符合单位高斯分布。（均值为0，方差，标准差都为1）

ps: 不是一个输入在多个神经元上高斯分布，而是所有输入在一个神经元上高斯分布，所以是纵向计算，但是实际的输出的维度还是不变

## 核心要点：单独看“方差为1”这个数字意义不大。它的核心价值在于作为一个“标准参照点”。
## 无论是将不同数据集标准化到该点进行比较，还是在理论中以该点为基础构建分布，方差为1都代表了一个去量纲化、中心化后的标准波动单位。

按照上面对每一列进行取平均值和标准差，就可以算出对应的 mini-batch 方差，并做归一化 (xi - xmean) / sqrt(方差 + epsilon)   epsilon防止除0


但是我们并不想总是让hpreact保持高斯分布，而是在初始化的时候才这样，所以我们引入g缩放，b平移，让后续神经网络自行调整。但是g初始化为1, b初始化为全0
这样在最开始，就是hpreact高斯分布的，只有在loss反向传播以后，才会改变g和b的值。

缩放平移 g * x + b 

o 是方差 u是平均值
g 是缩放 b是平移
                     neuron0   neuron1   neuron2                            neuron0 答案示例，1和2同理
x0[2,4,3]           [   3   ,    7    ,    2   ]                      [ g * (3 - u0) / sqrt(o0**2 + e) + b]
x1[1,3,4]   * W =>  [   2   ,    6    ,    1   ]  ==== 批量归一化 ===> [ g * (3 - u0) / sqrt(o0**2 + e) + b]
x2[5,9,8]           [   4   ,    5    ,    8   ]                      [ g * (3 - u0) / sqrt(o0**2 + e) + b]

注意，g和b是共享的，u0 o0是每个神经元独自计算的

50min30s --- 其实没有太懂原理，主要只能从他的描述和直觉来感受
这个批量归一化的过程可能会引入一些奇怪或者不自然的现象，因为同一批的输入，现在有了相互的耦合，因为要算在某个神经上的平均值，均方差
但是更奇怪的是，这种耦合在神经网络中被证明是有益的，因为你可以把这个看作是一种正则化？因为相当于对输入进行了所谓的增强。让神经网络难以过度拟合
所以，通过引入这些噪声，它实际上填充了样本并正则化了神经网络。这也让我们更难以一处批量归一化
提了其他几种归一化， 层归一化、实例归一化、组归一化等，但是不太容易，因为批量归一化效果太好了
'''

torch.

# 54min
# 我们希望在训练好一个模型后，能在环境中部署它，并且能够输入单个样本并且从我们的神经网络中得到结果。
# 但是根据之前的优化，forward的时候，求logits是要根据前面一批的输入才能被计算出来计算，这就和推理时的单个样本冲突
# 论文建议：在训练后，计算并且设置一次训练集上的批量归一化均值和标准差。 这个过程称之为： 校准批量归一化统计数据
# 方法1：
# 可以用with 语句来对一小段代码进行装饰
# 其实就是用整个训练集做embedding作为输入，hpreact = X @ W1 + b1
# 然后计算全量训练集的hpreact的结果的平均值和标准差
# 在推理的时候，计算loss时，就用这个固定的额值：hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias
# 
# 方法2
# 每个阶段以0.001的临时量来更新，这样就不用最后算一次了
#with torch.no_grad()
# bnmean = 0.999 * bnmean + 0.001 * bnmeani
# bnstd = 0.999 * bnstd + 0.001 * bnstdi
# 
# 1:01:00  解释归一化时的epsilon, 它默认很小 10^(-5),防止公式除以0
# X @ W1 + b1 ， bias在计算的时候实际上是被减掉了，所以没用
# 因为 hpreact = embcat @ W1 + b1, bnmeani = hpreact.mean(0, keepdim=True)
# 然后重新计算hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias ， (hpreact - bnmeani) 这个实际上是把b1又减去了
# b1这种偏移的功能现在已经被 bnbias替代了
# 1:03:00 总结：
"""
关于批量归一化层 BatchNormLayer 的简要总结：

我们使用批量归一化来控制神经网络中激活值的统计特性，在神经网络中广泛使用批量归一化层带来控制激活值的统计特性；通常，我们会在
包含乘法运算层（线性层或者卷积层）之后放置批量归一化层。批量归一化的内部有用于增益和偏差的参数，这些参数是通过反向传播
训练得到的。
它还有两个缓冲区，分别是运行时的均值和标准差。这些并不是反向传播训练而来，而是向“运行时的均值更新”来训练的

批量归一化层的步骤：
计算输入到激活值在该批次上的均值和标准差，并将该批次数据中心化，使其成为单位高斯分布，然后它通过学习到的偏差和增益对其进行偏移（bias）和缩放（gain）。
在此基础上， 批量归一化层还记录了输入的均值和标准差，并且它保持着这个运行中的均值和标准差。这会用在后续的推理过程中，而不用重新估计均值。

1:05:00
一个实际的例子：ResNet，它是个残差神经网络，提到了 bottleneck block。
算了，这个明天再看
"""
