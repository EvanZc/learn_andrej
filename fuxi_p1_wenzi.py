'''
1min - 10min
真正赋予神经网络能力的是 反向传播自动微分引擎

这节课要做的就是一个  基于自动微分引擎构建的神经网络库

10min
基于一个一元二次方程  3x**2 - 4x + 5
引出了求导公式 f(x + h) - f(x) / h  limit(h-> 0)
可导的条件也是上面的式子存在极限

18min
基于 d = a*b + c
分别用上面的求导公式求了d相对a、b、c的导数

25min左右是画图代码

30min
一个数相对自己本身的导数是1S
30min 30s
叶子节点是神经网络的权重，其他叶子节点或者中间节点是基础数据或者过程数据

我们要计算的是  损失函数相对于权重的导数，因为权重将会利用梯度信息进行迭代更新

34min -- 53min 基础地用公式手动计算相对于loss的导数S
手动利用上面的求导公式算了L相对与前面所有abcdef的导数并且手动填充
S
不过在42min左右引入了链式法则（chain rule）

疑问：
现在学到的都是tensor里面都是参数，然后 data += -learn_rate * grad
第一节课是如何把这个学习率传播下去的？

53min
把输入x比作神经的轴突 axon
把权重比喻成神经的突触 synapse
它俩集合到神经
把偏置比喻成神经自身对输入的反应

还要经过激活，激活就是把 wx+b 左右到激活函数，然它做一个从线性到非线性的变换，可能是压缩函数，比如tanh、ReLU等,得到最终的输出值

55min开始写对应代码


'''


'''
代码学到的
import numpy as np

np.arange(-5, 5, 0.25)


class里面
__init__ 初始化
__repr__ 打印
__add___ 
__mul___

__init__函数里面，用了空的元组来作为children
_children = ()

'''


'''
画图 
import matplotlib.pyplot as plt
plt.plot(xs, ys) --> xs就是f(x) , ys 就是函数的y值

25min
给出了基本的画图代码逻辑（这里就不单独学习了，因为暂时不重要）
'''